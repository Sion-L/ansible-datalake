{% set rm00 = groups['hadoop'][0] %}
{% set rm03 = groups['hadoop'][3] %}
<configuration>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yrc</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm00,rm03</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm00</name>
        <value>{{ hostvars[rm00].datalake_host }}</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm03</name>
        <value>{{ hostvars[rm03].datalake_host }}</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>{% for host in groups['zookeeper'] %}{{ hostvars[host].datalake_host }}:2181{% if not loop.last %},{% endif %}{% endfor %}</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>
    <property>
        <name>yarn.scheduler.fair.allocation.file</name>
        <value>{{ hadoop_conf_dir }}/fair-scheduler.xml</value>
    </property>
    <property>
        <name>yarn.scheduler.fair.preemption</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm00</name>
        <value>{{ hostvars[rm00].datalake_host }}:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm03</name>
        <value>{{ hostvars[rm03].datalake_host }}:8088</value>
    </property>
    <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>1</value>
        <description> Maximum percent of resources in the cluster which can be used to run application masters i.e. controls number of concurrent running applications. </description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>10240</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>4</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>3</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
        <value>2</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>8192</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>2</value>
    </property>
    <property>
        <name>yarn.scheduler.increment-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>259200</value>
    </property>
    <property>
        <name>mapreduce.map.output.compress</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>97</value>
    </property>
</configuration>
